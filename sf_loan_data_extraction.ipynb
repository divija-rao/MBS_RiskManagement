{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68dc2c26",
   "metadata": {},
   "source": [
    "*This Python script automates the extraction, cleaning, and consolidation of the extensive Freddie Mac Single-Family Loan-Level Dataset, a standard and widely-used resource in mortgage-backed securities analysis. Designed for computational efficiency, the code processes a decade of quarterly data (2014-2024) by systematically reading origination and performance files.Through a robust process of deduplication and merging, the script constructs a reliable panel dataset that serves as a critical input for forecasting loan performance and measuring portfolio risk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b37b3",
   "metadata": {},
   "source": [
    "### **Origination Data**\n",
    "\n",
    "#### Within a Quarter:\n",
    "The origination file for each quarter should contain unique Loan Sequence Number values within that quarter, as it typically records new loans originated during that period. \n",
    "\n",
    "#### Across Quarters:\n",
    "Origination data across quarters (e.g., Q1, Q2, Q3, Q4) is not necessarily unique. A loan originated in Q1 might appear again in Q2, Q3, or Q4 if:\n",
    " - The loan’s details were updated (e.g., due to corrections or refinancing).\n",
    " - The dataset includes historical origination data re-reported in later quarters for context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94411c5b",
   "metadata": {},
   "source": [
    "### **Performance Data**\n",
    "\n",
    "#### Within a Quarter:\n",
    "The performance file contains multiple entries for the same Loan Sequence Number within a quarter, reflecting monthly performance updates. For example, a loan might have records for Monthly Reporting Period values like 20240101 (January), 20240201 (February), and 20240301 (March) in Q1 2024. \n",
    "\n",
    "#### Across Quarters:\n",
    "Performance data spans multiple quarters, so a loan active in Q1, Q2, Q3, and Q4 will have a latest performance record in each quarter’s file (e.g., the last month of Q1, Q2, Q3, and Q4). This results in multiple records per loan ID across quarters unless deduplicated globally. For instance, a loan might have its latest Q1 record in March 2024, its latest Q2 record in June 2024, and so on, leading to up to 4 records (one per quarter) if no further consolidation occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b619b",
   "metadata": {},
   "source": [
    "### **Relationship Between Origination and Performance Data**\n",
    "\n",
    "#### Loan IDs in Performance vs. Origination: \n",
    "The performance data for a given quarter can include Loan Sequence Number values that are more than those in the origination data for the same quarter. This happens because:\n",
    "\n",
    "Performance data tracks all active loans, including those originated in previous quarters. For example, performance_2024Q2 includes loans originated in Q1 and Q2, while origination_2024Q2 only includes new loans from Q2.\n",
    "\n",
    "The inner merge (pd.merge(..., how='inner')) ensures that only loans with both origination and performance data are kept, but the performance data’s broader scope means some loan IDs might not match if origination data is missing or incomplete for earlier quarters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8aac61",
   "metadata": {},
   "source": [
    "### Data Extraction Code Approach\n",
    "\n",
    "- Process data quarterly across all years (2014-2024).\n",
    "- For origination data, keep the first instance of each Loan Sequence Number globally across all years and quarters.\n",
    "- For performance data, keep the last instance of each Loan Sequence Number globally across all years and quarters.\n",
    "- Use an inner join to merge origination and performance data.\n",
    "- Ensure computational efficiency given the large dataset size and quarterly separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ec08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c67d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the data folders and Excel layout file\n",
    "data_dir = '/Users/dr/Documents/GitHub/MBS_RiskManagement/data/'\n",
    "layout_file = '/Users/dr/Documents/GitHub/MBS_RiskManagement/READ_ME/SF LLD File Layout Release 44.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse sheet content from Excel\n",
    "def parse_sheet_from_excel(sheet_name):\n",
    "    df = pd.read_excel(layout_file, sheet_name=sheet_name, header=None)\n",
    "    columns = []\n",
    "    dtypes = {}\n",
    "    start_row = df.index[df[0].str.contains('FIELD POSITION', na=False)].tolist()\n",
    "    if start_row:\n",
    "        start_row = start_row[0] + 1\n",
    "    else:\n",
    "        start_row = 0\n",
    "    \n",
    "    for index, row in df.iloc[start_row:].iterrows():\n",
    "        if pd.isna(row[0]) or not isinstance(row[1], str):\n",
    "            break\n",
    "        attribute_name = row[1].strip()\n",
    "        data_type = row[2].strip() if pd.notna(row[2]) else 'object'\n",
    "        columns.append(attribute_name)\n",
    "        if 'Alpha' in data_type or 'Alpha Numeric' in data_type or '- PYYQnXXXXXXX' in data_type:\n",
    "            dtypes[attribute_name] = 'object'\n",
    "        elif 'Numeric' in data_type and not ' - ' in data_type:\n",
    "            dtypes[attribute_name] = 'Int64'\n",
    "        elif 'Numeric - ' in data_type:\n",
    "            dtypes[attribute_name] = 'float64'\n",
    "        elif 'Date' in data_type:\n",
    "            dtypes[attribute_name] = 'datetime64[ns]'\n",
    "        else:\n",
    "            dtypes[attribute_name] = 'object'\n",
    "    print(f\"{sheet_name} columns: {columns}\")\n",
    "    print(f\"{sheet_name} dtypes: {dtypes}\")\n",
    "    return columns, dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c4a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Origination and Performance sheets from Excel\n",
    "origination_columns, origination_dtypes = parse_sheet_from_excel('Origination Data File')\n",
    "performance_columns, performance_dtypes = parse_sheet_from_excel('Monthly Performance Data File')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the extracted_data folder \n",
    "extracted_data_dir = os.path.join('/Users/dr/Documents/GitHub/MBS_RiskManagement/', 'extracted_data') \n",
    "os.makedirs(extracted_data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parse_yyyymm(val):\n",
    "    \"\"\"Convert YYYYMM or YYYYMMDD strings into datetime.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return pd.NaT\n",
    "    val = str(val).strip()\n",
    "    if len(val) == 6:  # YYYYMM\n",
    "        return datetime.strptime(val, \"%Y%m\")\n",
    "    elif len(val) == 8:  # YYYYMMDD\n",
    "        return datetime.strptime(val, \"%Y%m%d\")\n",
    "    else:\n",
    "        return pd.to_datetime(val, errors=\"coerce\")  # fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years and quarters to process \n",
    "years = range(2014, 2025) # 2014 to 2024 inclusive \n",
    "quarters = ['Q1', 'Q2', 'Q3', 'Q4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1efd4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check available disk space \n",
    "def check_disk_space(path, required_mb=100): \n",
    "    stat = os.statvfs(path) \n",
    "    free_mb = (stat.f_bavail * stat.f_frsize) / (1024 * 1024) \n",
    "    if free_mb < required_mb: \n",
    "        print(f\"Warning: Only {free_mb:.1f} MB free on {path}. Need at least {required_mb} MB. Free up space or adjust output.\") \n",
    "        return False \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global dictionaries to track first origination and last performance \n",
    "origination_first = {} \n",
    "performance_last = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each year and quarter\n",
    "for year in years:\n",
    "    for quarter in quarters:\n",
    "        folder_path = os.path.join(data_dir, f'historical_data_{year}{quarter}')\n",
    "        \n",
    "        if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "            print(f\"Processing folder: {folder_path}\")\n",
    "            \n",
    "            # Check disk space before processing\n",
    "            if not check_disk_space(extracted_data_dir, required_mb=500):\n",
    "                break\n",
    "            \n",
    "            # ------------------------\n",
    "            # Process origination data\n",
    "            # ------------------------\n",
    "            origination_file_pattern = f'historical_data_{year}{quarter}.txt'\n",
    "            origination_file = os.path.join(folder_path, origination_file_pattern)\n",
    "\n",
    "            if os.path.exists(origination_file):\n",
    "                chunk_iterator = pd.read_csv(\n",
    "                    origination_file,\n",
    "                    sep='|',\n",
    "                    header=None,\n",
    "                    names=origination_columns,\n",
    "                    encoding='utf-8',\n",
    "                    dtype={col: 'object' for col in origination_columns},  # read everything as string first\n",
    "                    chunksize=100000,\n",
    "                    low_memory=False\n",
    "                )\n",
    "\n",
    "                for chunk in chunk_iterator:\n",
    "                    # Parse origination date columns\n",
    "                    for col in ['First Payment Date', 'Maturity Date']:\n",
    "                        if col in chunk.columns:\n",
    "                            chunk[col] = chunk[col].apply(parse_yyyymm)\n",
    "\n",
    "                    # Apply other dtypes\n",
    "                    for col, dtype in {k: v for k, v in origination_dtypes.items()\n",
    "                                       if k not in ['First Payment Date', 'Maturity Date']}.items():\n",
    "                        if col in chunk.columns:\n",
    "                            chunk[col] = chunk[col].astype(dtype, errors='ignore')\n",
    "\n",
    "                    # Keep FIRST occurrence per Loan ID (global)\n",
    "                    chunk = chunk.sort_values(['Loan Sequence Number', 'First Payment Date'])\n",
    "                    first_rows = chunk.groupby('Loan Sequence Number').head(1)\n",
    "                    for loan_id, row in first_rows.set_index('Loan Sequence Number').to_dict('index').items():\n",
    "                        if loan_id not in origination_first:\n",
    "                            origination_first[loan_id] = row\n",
    "            else:\n",
    "                print(f\"Origination file not found: {origination_file_pattern}\")\n",
    "\n",
    "            \n",
    "            # ------------------------\n",
    "            # Process performance data\n",
    "            # ------------------------\n",
    "            performance_file_pattern = f'historical_data_time_{year}{quarter}.txt'\n",
    "            performance_file = os.path.join(folder_path, performance_file_pattern)\n",
    "\n",
    "            if os.path.exists(performance_file):\n",
    "                chunk_iterator = pd.read_csv(\n",
    "                    performance_file,\n",
    "                    sep='|',\n",
    "                    header=None,\n",
    "                    names=performance_columns,\n",
    "                    encoding='utf-8',\n",
    "                    dtype={col: 'object' for col in performance_columns},  # read everything as string first\n",
    "                    chunksize=100000,\n",
    "                    low_memory=False\n",
    "                )\n",
    "        \n",
    "                for chunk in chunk_iterator:\n",
    "                    # Parse performance date columns\n",
    "                    for col in ['Monthly Reporting Period', 'Defect Settlement Date', \n",
    "                                'Zero Balance Effective Date', 'Due Date of Last Paid Installment (DDLPI)']:\n",
    "                        if col in chunk.columns:\n",
    "                            chunk[col] = chunk[col].apply(parse_yyyymm)\n",
    "\n",
    "                    # Apply other dtypes\n",
    "                    for col, dtype in {k: v for k, v in performance_dtypes.items()\n",
    "                                       if col not in ['Monthly Reporting Period', 'Defect Settlement Date',\n",
    "                                                      'Zero Balance Effective Date', 'Due Date of Last Paid Installment (DDLPI)']}.items():\n",
    "                        if col in chunk.columns:\n",
    "                            chunk[col] = chunk[col].astype(dtype, errors='ignore')\n",
    "\n",
    "                    # Keep LAST occurrence per Loan ID (global)\n",
    "                    chunk = chunk.sort_values(['Loan Sequence Number', 'Monthly Reporting Period'])\n",
    "                    last_rows = chunk.groupby('Loan Sequence Number').tail(1)\n",
    "                    performance_last.update(last_rows.set_index('Loan Sequence Number').to_dict('index'))\n",
    "            else:\n",
    "                print(f\"Performance file not found: {performance_file_pattern}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert global dictionaries to DataFrames\n",
    "df_origination = pd.DataFrame.from_dict(origination_first, orient='index')\n",
    "df_performance = pd.DataFrame.from_dict(performance_last, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d9669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join on Loan Sequence Number\n",
    "merged = df_origination.merge(df_performance, \n",
    "                              left_index=True, \n",
    "                              right_index=True, \n",
    "                              how='inner', \n",
    "                              suffixes=('_orig', '_perf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b910e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save final merged dataset\n",
    "output_file = os.path.join(extracted_data_dir, \"merged_loans_2014_2024.csv\")\n",
    "merged.to_csv(output_file, index=True)\n",
    "\n",
    "print(f\"Final merged dataset written to: {output_file}\")\n",
    "print(f\"Total loans in merged set: {len(merged)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
